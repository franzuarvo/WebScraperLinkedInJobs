# -*- coding: utf-8 -*-
"""WebScraperLinkedInJobs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bd-BpebfoMgopqEo5spmuwqsH79PxTKc
"""

#@title primero se descarga el entorno donde se va a guardar nuestra data, en REDIS,
# Instalar Redis Server y redis-py
!apt-get install redis-server > /dev/null
!pip install redis > /dev/null

# Iniciar el servidor de Redis
import os
os.system("redis-server --daemonize yes")

#@title esta es una libreria de web scraper ya existente que hace web scraper de linkedin y indeed perú
!pip install -U python-jobspy

#@title para poder hacer web scraping necesitamos esta libreria y unidecode para poder pasarlo a un formato normal de texto
!pip install requests beautifulsoup4 unidecode

#@title Se hace web scraping a linkedin jobs perú (puestos publicados del día) y se almacena en Redis (scrap_4_Data)
import requests
from bs4 import BeautifulSoup
from unidecode import unidecode
from datetime import datetime
import re
import json
import redis

# Configuración de Redis
REDIS_HOST = 'localhost'
REDIS_PORT = #complete here
REDIS_PASSWORD = #complete here

# Función para almacenar datos en Redis
def store_data_in_redis(scraper_name, data):
    """
    Almacena datos en Redis con una clave específica para cada scraper.

    Args:
        scraper_name (str): El nombre del scraper (para identificar los datos).
        data (list or dict): Los datos a almacenar en formato JSON.
    """
    try:
        # Conectar a Redis
        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD)

        # Convertir los datos a formato JSON
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        data_json = json.dumps({
            "timestamp": timestamp,
            "data": data
        }, default=str, indent=4)

        # Crear una clave única para el scraper
        redis_key = f"{scraper_name}_data"

        # Guardar en Redis
        r.set(redis_key, data_json)

        print(f"Datos del scraper '{scraper_name}' almacenados correctamente en Redis bajo la clave '{redis_key}'.")

    except Exception as e:
        print(f"Error al almacenar datos en Redis: {e}")

# URL base y parámetros para el scraping
LI_JOB_URL = "https://www.linkedin.com/jobs/search/"
PERU_GEO_ID = "102927786"

def transform_obj_to_params(obj):
    return "&".join([f"{key}={value}" for key, value in obj.items()])

def to_snake_case(text):
    return re.sub(r'(?<!^)(?=[A-Z])', '_', text).replace(" ", "_").lower()

def get_jobs(params, max_pages=5):
    """Recupera trabajos de LinkedIn con paginación hasta `max_pages`."""
    params["geoId"] = PERU_GEO_ID
    params["f_TPR"] = "r86400"  # Filtrar solo trabajos publicados en las últimas 24 horas
    jobs_data = []

    fecha_hoy = datetime.now().date()

    for page in range(max_pages):
        params["start"] = page * 25  # LinkedIn muestra 25 trabajos por página
        url = LI_JOB_URL + "?" + transform_obj_to_params(params)
        response = requests.get(url)
        if response.status_code != 200:
            continue

        parsers = BeautifulSoup(response.text, "html.parser")
        jobs = parsers.select(".jobs-search__results-list li")

        for job in jobs:
            div = job.select_one("div")
            if not div or 'data-entity-urn' not in div.attrs:
                continue

            fecha_creacion_elem = job.select_one(".job-search-card__listdate")
            fecha_creacion = (
                fecha_creacion_elem["datetime"]
                if fecha_creacion_elem and "datetime" in fecha_creacion_elem.attrs
                else None
            )

            # Confirmamos que la fecha de publicación sea hoy
            if fecha_creacion:
                fecha_publicacion = datetime.fromisoformat(fecha_creacion).date()
                if fecha_publicacion != fecha_hoy:
                    continue

            empresa_nombre = job.select_one(".hidden-nested-link")
            empresa_nombre = empresa_nombre.text.strip() if empresa_nombre else "Confidential"

            puesto = re.sub(r'\s+', ' ', unidecode(job.select_one("h3").text)).strip()

            job_data = {
                "urn": div["data-entity-urn"],
                "id": div["data-entity-urn"].split(":")[-1],
                "puesto": puesto,
                "enlace": job.select_one("a")["href"],
                "lugar": re.sub(r'\s+', ' ', unidecode(job.select_one(".job-search-card__location").text)).strip(),
                "fecha_creacion": fecha_creacion,
                "tiempo_relativo": re.sub(r'\s+', ' ', unidecode(fecha_creacion_elem.text)).strip() if fecha_creacion_elem else None,
                "empresa": {
                    "logo": job.select_one(".search-entity-media img")["data-delayed-url"] if job.select_one(".search-entity-media img") else None,
                    "enlaces": job.select_one(".hidden-nested-link")["href"].split("?")[0] if job.select_one(".hidden-nested-link") else None,
                    "enlace_trabajos": job.select_one(".hidden-nested-link")["href"] if job.select_one(".hidden-nested-link") else None,
                    "nombre": empresa_nombre
                }
            }
            jobs_data.append(job_data)

    return jobs_data

def get_job_urls(jobs_data):
    """Obtiene las URLs de los trabajos encontrados."""
    return [job["enlace"] for job in jobs_data]

# Parámetros de búsqueda sin filtro de palabras clave
params = {
    "keywords": "",  # Búsqueda general sin palabras clave específicas
    "f_E": "1,2,3,4,5"  # Todos los niveles de experiencia
}

# Obtener trabajos y convertir a JSON
jobs_data = get_jobs(params)

# Guardar los datos en Redis usando la función centralizada
store_data_in_redis("scraper_4", jobs_data)

# Imprimir resultados
fecha_actual = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print(f"Resultados del scraping ejecutado el {fecha_actual}\n")

jobs_json = json.dumps(jobs_data, default=str, indent=4)
print(f"Total de trabajos encontrados hoy: {len(jobs_data)}\n")

print("Trabajos encontrados hoy en formato JSON:\n")
print(jobs_json)

print("\nURLs de los trabajos encontrados hoy:")
for url in get_job_urls(jobs_data):
    print(url)

#@title se hace web scraping de indeed y linkedin perú a todos los puestos que se publicaron hoy día (scrap_3)
import redis
from jobspy import scrape_jobs
import json
import pandas as pd
from datetime import datetime

# Configuración de Redis
REDIS_HOST = 'localhost'
REDIS_PORT = #complete here
REDIS_PASSWORD = #complete here

# Conexión a Redis
redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True)

def buscar_trabajos(site_names, search_term, location, results_wanted=50, hours_old=24, country_indeed=None):
    """
    Realiza la búsqueda de trabajos sin filtrar los campos.

    :param site_names: Lista de sitios donde buscar (por ejemplo: ["indeed", "linkedin"])
    :param search_term: El término de búsqueda de trabajo (por ejemplo: "Gerente")
    :param location: La ubicación de búsqueda (por ejemplo: "Peru")
    :param results_wanted: Número de resultados deseados (default: 50)
    :param hours_old: Máxima antigüedad de los resultados en horas (default: 24)
    :param country_indeed: País para búsquedas específicas de Indeed (opcional)

    :return: Resultados de búsqueda sin filtrar en formato JSON.
    """
    try:
        jobs = scrape_jobs(
            site_name=site_names,
            search_term=search_term,
            location=location,
            results_wanted=results_wanted,
            hours_old=hours_old,
            country_indeed=country_indeed
        )
        return jobs.to_json(orient='records', force_ascii=False, indent=4)
    except Exception as e:
        print(f"Error al buscar trabajos: {e}")
        return json.dumps([])

def filtrar_trabajos(jobs_json):
    """
    Filtra solo los campos relevantes de los resultados de trabajos y ajusta las URLs.

    :param jobs_json: Resultados de trabajos en formato JSON.

    :return: Resultados filtrados en formato JSON.
    """
    # Cargar los datos JSON en un DataFrame
    jobs = pd.read_json(jobs_json)

    if jobs.empty:
        print("No se encontraron trabajos para los criterios especificados.")
        return json.dumps([])

    # Filtrar solo los campos relevantes
    jobs_filtrados = jobs[[
        'site',
        'job_url',
        'job_url_direct',
        'title',
        'company',
        'location',
        'company_url',
        'company_url_direct',
        'company_addresses',
        'company_num_employees'
    ]]

    # Reemplazar las barras invertidas en las URLs
    for col in ['job_url', 'job_url_direct', 'company_url', 'company_url_direct']:
        jobs_filtrados[col] = jobs_filtrados[col].str.replace(r'\\/', '/', regex=True)

    # Convertir el DataFrame filtrado a JSON
    jobs_json_filtrado = jobs_filtrados.to_json(orient='records', force_ascii=False, indent=4)

    return jobs_json_filtrado

def guardar_en_redis(clave, datos_json):
    """
    Guarda los datos en Redis bajo una clave específica.

    :param clave: La clave bajo la cual se guardarán los datos.
    :param datos_json: Los datos en formato JSON.
    """
    try:
        redis_client.set(clave, datos_json)
        print(f"Datos guardados en Redis con la clave: {clave}")
    except Exception as e:
        print(f"Error al guardar en Redis: {e}")

# Ejemplo de uso
if __name__ == "__main__":
    # Llamada para buscar trabajos publicados hoy en LinkedIn e Indeed Perú
    trabajos_json = buscar_trabajos(
        site_names=["indeed", "linkedin"],
        search_term="Ingeniero de Software",
        location="Peru",
        results_wanted=50,
        hours_old=24,  # Solo trabajos de las últimas 24 horas
        country_indeed='Peru'
    )

    # Llamada para filtrar trabajos
    trabajos_filtrados_json = filtrar_trabajos(trabajos_json)

    # Guardar los resultados en Redis con la clave fija 'scrap_3'
    clave_redis = "scrap_3"
    guardar_en_redis(clave_redis, trabajos_filtrados_json)

    # Imprimir resultados
    trabajos_filtrados = json.loads(trabajos_filtrados_json)
    for job in trabajos_filtrados:
        print(f"\nTítulo: {job['title']}")
        print(f"Empresa: {job['company']}")
        print(f"Ubicación: {job['location']}")
        print(f"Link del Trabajo: {job['job_url']}")
        print(f"Link Directo del Trabajo: {job['job_url_direct']}")
